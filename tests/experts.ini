;; Transformer

[main]
name="Mixture of NMT Experts"
tf_manager=<tf_manager>
output="tests/outputs/experts"
overwrite_output_dir=True
batch_size=16
epochs=1
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner_uniform>, <runner_global>, <runner_context>]
postprocess=None
evaluation=[("target_context", "target", evaluators.BLEU)]
logging_period=10
validation_period=30
random_seed=1234

[tf_manager]
class=tf_manager.TensorFlowManager
num_sessions=1
num_threads=4

[train_data]
class=dataset.load
series=["source", "target"]
data=["tests/data/train.tc.en", "tests/data/train.tc.de"]

[val_data]
class=dataset.load
series=["source", "target"]
data=["tests/data/val.tc.en", "tests/data/val.tc.de"]

[encoder_vocabulary]
class=vocabulary.from_wordlist
path="tests/data/encoder_vocab.tsv"

[decoder_vocabulary]
class=vocabulary.from_wordlist
path="tests/data/decoder_vocab.tsv"

[inpseq]
class=model.sequence.EmbeddedSequence
name="input"
embedding_size=6
max_length=7
data_id="source"
vocabulary=<encoder_vocabulary>

[encoder_transformer]
class=encoders.transformer.TransformerEncoder
input_sequence=<inpseq>
ff_hidden_size=10
depth=2
n_heads=3
dropout_keep_prob=0.9

[decoder_transformer]
class=decoders.transformer.TransformerDecoder
encoders=[<encoder_transformer>]
dropout_keep_prob=0.5
data_id="target"
max_output_len=3
vocabulary=<decoder_vocabulary>
embedding_size=6
ff_hidden_size=10
depth=2
n_heads_self=3
n_heads_enc=2

[encoder_rnn]
class=encoders.recurrent.SentenceEncoder
rnn_size=7
max_input_len=10
embedding_size=11
data_id="source"
vocabulary=<encoder_vocabulary>

[attention_rnn]
class=attention.Attention
encoder=<encoder_rnn>

[decoder_rnn]
class=decoders.decoder.Decoder
encoders=[<encoder_rnn>]
rnn_size=8
embedding_size=9
attentions=[<attention_rnn>]
output_projection=<dec_maxout_output>
dropout_keep_prob=0.5
data_id="target"
max_output_len=10
vocabulary=<decoder_vocabulary>
supress_unk=True

[dec_maxout_output]
class=decoders.output_projection.maxout_output
maxout_size=9

[mixture_uniform]
class=decoders.experts.ExpertDecoder
decoders=[<decoder_transformer>, <decoder_rnn>]
vocabulary=<decoder_vocabulary>
data_id="target"
max_output_len=10
gating_strategy="uniform"

[mixture_global]
class=decoders.experts.ExpertDecoder
decoders=[<decoder_transformer>, <decoder_rnn>]
vocabulary=<decoder_vocabulary>
data_id="target"
max_output_len=10
gating_strategy="global"

[mixture_context]
class=decoders.experts.ExpertDecoder
decoders=[<decoder_transformer>, <decoder_rnn>]
vocabulary=<decoder_vocabulary>
data_id="target"
max_output_len=10
gating_strategy="context-aware"
gate_dimension=11

[trainer]
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<mixture_uniform>, <mixture_global>, <mixture_context>]
optimizer=<lazyadam_g>

[decayed_lr]
class=functions.noam_decay
learning_rate=0.2
model_dimension=6
warmup_steps=111

[lazyadam_g]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.98
epsilon=1.0e-9
learning_rate=<decayed_lr>

[runner_uniform]
class=runners.GreedyRunner
decoder=<mixture_uniform>
output_series="target_uniform"

[runner_global]
class=runners.GreedyRunner
decoder=<mixture_global>
output_series="target_global"

[runner_context]
class=runners.GreedyRunner
decoder=<mixture_context>
output_series="target_context"
